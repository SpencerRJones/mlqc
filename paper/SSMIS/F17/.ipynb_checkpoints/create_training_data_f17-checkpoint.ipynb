{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d857a29-9e0a-4c81-93a9-08dc5abd72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates training data for F17 channel predictor\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from util_funcs.L1C import scantime2datetime\n",
    "from util_funcs import data2xarray, array_funcs\n",
    "import geography\n",
    "\n",
    "import sensor_info\n",
    "import local_functions\n",
    "\n",
    "satellite = sensor_info.satellite\n",
    "sensor = sensor_info.sensor\n",
    "\n",
    "datapath = f'/edata1/archive/GPM/1C_{satellite}_V7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671040bc-2bfa-413c-ab81-01c896fb37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   1%|â–ˆ                                                                                                      | 2/200 [00:17<32:00,  9.70s/it]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Satellite: F13\n",
    "\n",
    "Get data and quality check for training.\n",
    "\n",
    "nscans: generally around 3220\n",
    "npixs: 90\n",
    "\n",
    "    \n",
    "Tb array will be set up as follows:\n",
    "    Tbs =  [m x n], where m is the number of samples and n is the\n",
    "           number of channels (features)\n",
    "    1-2:   19V and H\n",
    "    3:     22V\n",
    "    4-5:   37V and H\n",
    "    6-9:   92V and H (high-frequency channels are double sampled)\n",
    "    10-11: 150H\n",
    "    12-13: 183+-1 H\n",
    "    14-15: 183+-3 H\n",
    "    16-17: 183+-7 H\n",
    "'''\n",
    "\n",
    "#Set up a random seed for reproducibility\n",
    "np.random.seed(12)\n",
    "\n",
    "#Get a random list of files\n",
    "file_list = glob.glob(f'{datapath}*/*/1C.{satellite}.{sensor}.*.HDF5'); file_list.sort()\n",
    "\n",
    "#Everything looks good before about 04/2016:\n",
    "good_files = [ifile for ifile in file_list if int(ifile.split('/')[5]) <= 1604]\n",
    "\n",
    "flist = np.random.choice(good_files, size=200)\n",
    "\n",
    "#Loop through files and get good quality data.\n",
    "for i, ifile in enumerate(tqdm(flist, desc=\"Processing Files\")):\n",
    "\n",
    "    #print(f'{i+1} of {len(flist)}, {ifile}')\n",
    "\n",
    "    data = local_functions.read_ssmis_l1c(ifile)\n",
    "\n",
    "    lat = data['lat']\n",
    "    lon = data['lon']\n",
    "    scantime = data['scantime']\n",
    "    Tbs = data['Tbs']\n",
    "    qual = data['qual']\n",
    "\n",
    "    #Get only good quality data and reshape:\n",
    "    goodqual = np.all(qual == 0, axis=2)\n",
    "    all_bad = np.all(goodqual == False)\n",
    "    if all_bad:\n",
    "        print('all were bad.')\n",
    "        continue\n",
    "    lat = lat[goodqual]\n",
    "    lon = lon[goodqual]\n",
    "    scantime = scantime[np.where(goodqual)[0]]\n",
    "    Tbs = Tbs[goodqual]\n",
    "\n",
    "    #Check for NaNs (shouldn't be any if all good, but I've seen some)\n",
    "    nonans = local_functions.find_nan_rows(Tbs, return_good=True)\n",
    "    Tbs = Tbs[nonans]\n",
    "    lat = lat[nonans]\n",
    "    lon = lon[nonans]\n",
    "    scantime = scantime[nonans]\n",
    "    \n",
    "\n",
    "    #Attach GPROF surface map data to each pixel\n",
    "    sfctype = local_functions.attach_gpm_sfctype(lat, lon, scantime, sensor=sensor)\n",
    "\n",
    "    npixs = lat.size\n",
    "\n",
    "    #Output as NetCDF\n",
    "    dset = data2xarray(data_vars = [lat, lon, scantime, sfctype, Tbs],\n",
    "                       var_names = ['latitude', 'longitude', 'scantime', 'sfctype', 'Tbs'],\n",
    "                       dims = [npixs, sensor_info.nfeatures],\n",
    "                       dim_names = ['pixels', 'channels'])\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        training_dataset = dset\n",
    "    else:\n",
    "        training_dataset = xr.concat((training_dataset, dset), dim='pixels')\n",
    "\n",
    "training_dataset.to_netcdf(f'training_data/{satellite}_training_data.nc', engine='netcdf4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f6ebb-9533-4d00-a64e-9addd24cbd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
