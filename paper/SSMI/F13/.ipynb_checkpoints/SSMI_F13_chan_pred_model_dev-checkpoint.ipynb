{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ed9863-7a3a-49f2-b4eb-3f1cb466f908",
   "metadata": {},
   "source": [
    "# Channel Prediction Model for Quality Control\n",
    "\n",
    "This notebook contains the code and outlines the development of the\n",
    "SSMI Channel Prediction Neural Network for QC purposes of SSMI L1C\n",
    "data. The simple Feed-Forward Neural Networks (FFNNs) are trained on\n",
    "good quality SSMIS brightness temperature observation vectors and are\n",
    "trained to predict one channel from all the others as predictors.\n",
    "\n",
    "The end product is a tree of FFNNs, discretized by surface (ocean or \n",
    "land), and by channel, since these are fully distinct problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4384f412-5ad6-4c72-8376-a27a864600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import cartopy.crs as ccrs\n",
    "from util_funcs.L1C import scantime2datetime\n",
    "from util_funcs import data2xarray, array_funcs\n",
    "import geography\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#General parameters:\n",
    "\n",
    "satellite = 'F13'\n",
    "sensor = 'SSMI'\n",
    "\n",
    "nchans = 9\n",
    "batch_size = 1000\n",
    "input_size = nchans - 1\n",
    "hidden_size = 256\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d6d779-b40f-41b0-a040-bfbcfdb0e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Definitions:\n",
    "'''\n",
    "\n",
    "\n",
    "# Tb array will be set up as follows:\n",
    "#     Tbs =  [m x n], where m is the number of samples and n is the\n",
    "#            number of channels (features)\n",
    "#     1-2:   19.35 V and H\n",
    "#     3:     22.235 V\n",
    "#     4-5:   37.0 V and H\n",
    "#     6-9:   85.5 V and H #Double-sampled 85\n",
    "\n",
    "chan_desc = np.array(['19V', '19H', '22V', '37V', '37H', '85Va', '85Ha', '85Vb', '85Hb'])\n",
    "\n",
    "\n",
    "def extract_channel(Tb_array, chan):\n",
    "\n",
    "    '''\n",
    "    Use in preparing training data.\n",
    "\n",
    "    Assumes Tb array is [m x n] where m (rows) are samples\n",
    "    and n (columns) is the number of channels.\n",
    "\n",
    "    Passing in the channel description splits the data so\n",
    "    that the specified channel is its own vector y and the\n",
    "    rest are kept as predictors x.\n",
    "\n",
    "    Inputs:\n",
    "        Tb_array    |  ndarray of Tbs\n",
    "        chan        |  string of channel name\n",
    "    Outputs:\n",
    "        x           |  matrix of predictors (other channels)\n",
    "        y           |  vector of predictands (the missing channel)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    chan_desc = np.array(['19V', '19H', '22V', '37V', '37H', '85Va', '85Ha', '85Vb', '85Hb'])\n",
    "\n",
    "\n",
    "    chan_indx = np.where(chan == chan_desc)[0]\n",
    "\n",
    "    if np.size(chan_indx) == 0:\n",
    "        raise ValueError(f'Channel description must be in list {chan_desc}.')\n",
    "\n",
    "    y = Tb_array[:,chan_indx]\n",
    "    x = Tb_array[:,np.delete(np.arange(0,len(chan_desc)),chan_indx)]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "def split_data_indcs(x, train=80, test=10, val=10, device=None, randomize=False):\n",
    "\n",
    "    '''\n",
    "    Creates train/test/validation split for training data. Default is 80%/10%/10%.\n",
    "\n",
    "    Inputs:\n",
    "        x       |   Array of predictors. Expects shape (nsamples, nfeatures).\n",
    "        y       |   Array of predictands. Expects shape (nsamples, nfeatures).\n",
    "        train   |   Percentage of data to be used for training.\n",
    "        test    |   Percentage of data to be used for testing.\n",
    "        val     |   Percentage of data to be used for validation.\n",
    "                        -Train + test + val must equal 100.\n",
    "        device (optional)    |  Either 'cuda' or 'cpu'. \n",
    "        randomize (optional) |  Whether to shuffle the data before creating\n",
    "                                    the splits. This functionality is currently\n",
    "                                    not supported but is intended to be \n",
    "                                    implemented in the future.\n",
    "    '''\n",
    "\n",
    "    if train + test + val != 100:\n",
    "        raise ValueError(f'train + test + val must equal 100%.')\n",
    "\n",
    "    # if x.shape[0] != y.shape[0]:\n",
    "    #     raise ValueError(f'Dimensions of x {x.shape} and y {y.shape} not compatible.')\n",
    "\n",
    "    nsamples = x.shape[0]\n",
    "\n",
    "    ntrain = int(nsamples * (train / 100.))\n",
    "    ntest  = int(nsamples * (test / 100.))\n",
    "    nval   = nsamples - ntrain - ntest\n",
    "\n",
    "    indcs = np.arange(0,nsamples)\n",
    "\n",
    "    train_indcs = indcs[0:ntrain]\n",
    "    test_indcs  = indcs[ntrain:ntrain+ntest]\n",
    "    val_indcs   = indcs[ntrain+ntest:]\n",
    "\n",
    "    return train_indcs, test_indcs, val_indcs\n",
    "\n",
    "\n",
    "def train_model(model, nepochs, dataloader, learning_rate=0.001, quiet=False, stage=None, validation_dataloader=None):\n",
    "\n",
    "    nbatches = len(dataloader)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs)\n",
    "    device = None\n",
    "\n",
    "    if stage:\n",
    "        print(f'Training stage: {stage}')\n",
    "\n",
    "    loss_arr    = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    valloss_arr = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for i, (profs, obs) in enumerate(dataloader):\n",
    "            if device:\n",
    "                profs, obs = profs.to(device), obs.to(device)\n",
    "\n",
    "            if validation_dataloader and i%100==0:\n",
    "                valprofs, valobs = next(enumerate(validation_dataloader))[1]\n",
    "                val_pred = model(valprofs)\n",
    "                valloss  = criterion(val_pred, valobs)\n",
    "                print(f'Validation Loss = {valloss.item():.3f}')\n",
    "\n",
    "            #Forward pass:\n",
    "            obs_pred = model(profs)\n",
    "            loss     = criterion(obs_pred, obs)\n",
    "\n",
    "\n",
    "            #Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #Update neurons:\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_arr[i,epoch] = loss.item()\n",
    "            valloss_arr[i,epoch] = valloss.item()\n",
    "            \n",
    "            if not quiet:\n",
    "                if i%100 == 0:\n",
    "                    print(f'Epoch = {epoch+1}, batch = {i} of {nbatches}, loss = {loss.item():.3f}, LR = {scheduler.get_last_lr()[0]}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return loss_arr, valloss_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7793f0-89d3-461c-85cc-a264593eec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class Definitions:\n",
    "'''\n",
    "\n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.predictors = x\n",
    "        self.predictands = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.predictors)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.predictors[idx], self.predictands[idx]\n",
    "\n",
    "class channel_predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(channel_predictor, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l4 = nn.Linear(hidden_size, output_size)\n",
    "        self.actvn = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.actvn(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.actvn(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.actvn(out)\n",
    "        out = self.l4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcb625-aa57-45c8-90ab-879225725d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ffa4c5b3-ee1d-4e83-8644-047eefce5b48",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "SSMI CHANNEL PREDICTION MODEL:\n",
    "    1: Ocean\n",
    "\n",
    "'''\n",
    "\n",
    "sfc = [1]\n",
    "\n",
    "with xr.open_dataset(f'training_data/{satellite}_training_data.nc') as f:\n",
    "    \n",
    "    sfctype = f.sfctype.values\n",
    "\n",
    "    correct_sfc = np.isin(sfctype, sfc)\n",
    "    sfcindcs = np.where(correct_sfc)[0]\n",
    "\n",
    "    Tbs = f.Tbs.values[sfcindcs]\n",
    "\n",
    "#---Don't shuffle data before splitting\n",
    "#np.random.seed(40)\n",
    "#Tbs = array_funcs.shuffle_data(Tbs, axis=0)\n",
    "\n",
    "if Tbs.shape[0] > 5.0e+06:\n",
    "    Tbs = Tbs[:5_000_000,:]\n",
    "\n",
    "\n",
    "print(Tbs.shape)\n",
    "\n",
    "#---Split data into train/test/val:\n",
    "train_indcs, test_indcs, val_indcs = split_data_indcs(Tbs)\n",
    "\n",
    "Tbs_train = Tbs[train_indcs]\n",
    "Tbs_test  = Tbs[test_indcs]\n",
    "Tbs_val   = Tbs[val_indcs]\n",
    "\n",
    "#---Shuffle before converting to tensors:\n",
    "np.random.seed(40)\n",
    "Tbs_train = array_funcs.shuffle_data(Tbs_train, axis=0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55a1f014-e04e-47db-838b-3f20b297af62",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "'''\n",
    "Predict channels: Train all models\n",
    "'''\n",
    "\n",
    "for ichan, channel in enumerate(chan_desc):\n",
    "\n",
    "    #---Extract channel, x = predictors, y = channel to predict\n",
    "    x_train, y_train = extract_channel(Tbs_train, channel)\n",
    "    x_test,  y_test  = extract_channel(Tbs_test, channel)\n",
    "    x_val,   y_val   = extract_channel(Tbs_val, channel)\n",
    "    \n",
    "    x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "    x_test,  y_test  = torch.tensor(x_test), torch.tensor(y_test)\n",
    "    x_val,   y_val   = torch.tensor(x_val), torch.tensor(y_val)\n",
    "\n",
    "\n",
    "    #---Set up dataloaders:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset(x_train,y_train), \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=dataset(x_test,y_test), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=dataset(x_val,y_val), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "\n",
    "    #---Create model:\n",
    "    model = channel_predictor(input_size, hidden_size, output_size)\n",
    "\n",
    "    #---Train_model:\n",
    "    nbatches = len(train_loader)\n",
    "    nepochs_stage1 = 5\n",
    "    nepochs_stage2 = 10\n",
    "    nepochs_stage3 = 20\n",
    "\n",
    "    loss_stage1, valloss_stage1 = train_model(model, nepochs=5, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=1, validation_dataloader=val_loader)\n",
    "    loss_stage2, valloss_stage2 = train_model(model, nepochs=10, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=2, validation_dataloader=val_loader)\n",
    "    loss_stage3, valloss_stage3 = train_model(model, nepochs=20, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=3, validation_dataloader=val_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/{sensor}_{satellite}_channel_predictor_{channel}_ocean.pt')\n",
    "\n",
    "    loss_data = data2xarray(data_vars = (loss_stage1, loss_stage2, loss_stage3, \n",
    "                                     valloss_stage1, valloss_stage2, valloss_stage3),\n",
    "                        var_names = ('LossStage1','LossStage2','LossStage3',\n",
    "                                     'ValidationLossStage1', 'ValidationLossStage2', 'ValidationLossStage3'),\n",
    "                        dims = (nbatches,nepochs_stage1,nepochs_stage2, nepochs_stage3),\n",
    "                        dim_names = ('training_batches', 'epochs_stage1', 'epochs_stage2', 'epochs_stage3'))\n",
    "\n",
    "    loss_data.to_netcdf(f'diagnostics/loss_data_{channel}_ocean.nc', engine='netcdf4')\n",
    "\n",
    "    print(f'Finished training model for channel {channel}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bbfc1f-8aa2-4772-a0de-2151e455d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General parameters:\n",
    "nchans = 9\n",
    "batch_size = 1000\n",
    "input_size = nchans - 1 + 1 #Remaining channels + surface type\n",
    "hidden_size = 256\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286d562f-2684-4b6a-ac79-0aa7c9a343b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "SSMI CHANNEL PREDICTION MODEL:\n",
    "    2: Non-Ocean Surfaces\n",
    "\n",
    "'''\n",
    "\n",
    "with xr.open_dataset(f'training_data/{satellite}_training_data.nc') as f:\n",
    "    \n",
    "    sfctype = f.sfctype.values\n",
    "\n",
    "    correct_sfc = sfctype > 1\n",
    "\n",
    "    Tbs = f.Tbs.values[correct_sfc,:]\n",
    "    sfctype = sfctype[correct_sfc]\n",
    "\n",
    "#---Split data into train/test/val:\n",
    "train_indcs, test_indcs, val_indcs = split_data_indcs(Tbs)\n",
    "\n",
    "Tbs_train = Tbs[train_indcs]\n",
    "Tbs_test  = Tbs[test_indcs]\n",
    "Tbs_val   = Tbs[val_indcs]\n",
    "\n",
    "sfctype_train = sfctype[train_indcs].astype(np.float32)\n",
    "sfctype_test  = sfctype[test_indcs].astype(np.float32)\n",
    "sfctype_val   = sfctype[val_indcs].astype(np.float32)\n",
    "\n",
    "#---Shuffle before converting to tensors:\n",
    "np.random.seed(40)\n",
    "Tbs_train, shuffled_indcs = array_funcs.shuffle_data(Tbs_train, axis=0, return_indcs=True)\n",
    "sfctype_train = sfctype_train[shuffled_indcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a135f0-542e-47c9-ace0-4d035ab8bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11.,  8., 18., ..., 10.,  4., 11.], shape=(4007694,), dtype=float32),\n",
       " array([ 5.,  5., 11., ...,  3., 17.,  3.], shape=(500961,), dtype=float32),\n",
       " array([ 3.,  3.,  3., ..., 11., 11., 11.], shape=(500963,), dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfctype_train, sfctype_test, sfctype_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cc489-288d-473b-ab3d-c3708b1484f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predict channels: Train all models\n",
    "'''\n",
    "\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "for ichan, channel in enumerate(chan_desc):\n",
    "\n",
    "    #---Extract channel, x = predictors, y = channel to predict\n",
    "    x_train, y_train = extract_channel(Tbs_train, channel)\n",
    "    x_test,  y_test  = extract_channel(Tbs_test, channel)\n",
    "    x_val,   y_val   = extract_channel(Tbs_val, channel)\n",
    "\n",
    "    x_train = np.concatenate((x_train, sfctype_train[:,None]), axis=1)\n",
    "    x_test  = np.concatenate((x_test,  sfctype_test[:,None]), axis=1)\n",
    "    x_val   = np.concatenate((x_val,   sfctype_val[:,None]), axis=1)\n",
    "    \n",
    "    x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "    x_test,  y_test  = torch.tensor(x_test), torch.tensor(y_test)\n",
    "    x_val,   y_val   = torch.tensor(x_val), torch.tensor(y_val)\n",
    "\n",
    "\n",
    "    #---Set up dataloaders:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset(x_train,y_train), \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=dataset(x_test,y_test), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=dataset(x_val,y_val), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "\n",
    "    #---Create model:\n",
    "    model = channel_predictor(input_size, hidden_size, output_size)\n",
    "\n",
    "    #---Train_model:\n",
    "    nbatches = len(train_loader)\n",
    "    nepochs_stage1 = 5\n",
    "    nepochs_stage2 = 10\n",
    "    nepochs_stage3 = 20\n",
    "\n",
    "    loss_stage1, valloss_stage1 = train_model(model, nepochs=5, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=1, validation_dataloader=val_loader)\n",
    "    loss_stage2, valloss_stage2 = train_model(model, nepochs=10, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=2, validation_dataloader=val_loader)\n",
    "    loss_stage3, valloss_stage3 = train_model(model, nepochs=20, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=3, validation_dataloader=val_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/GMI_channel_predictor_{channel}_nonocean.pt')\n",
    "\n",
    "    loss_data = data2xarray(data_vars = (loss_stage1, loss_stage2, loss_stage3, \n",
    "                                     valloss_stage1, valloss_stage2, valloss_stage3),\n",
    "                        var_names = ('LossStage1','LossStage2','LossStage3',\n",
    "                                     'ValidationLossStage1', 'ValidationLossStage2', 'ValidationLossStage3'),\n",
    "                        dims = (nbatches,nepochs_stage1,nepochs_stage2, nepochs_stage3),\n",
    "                        dim_names = ('training_batches', 'epochs_stage1', 'epochs_stage2', 'epochs_stage3'))\n",
    "\n",
    "    loss_data.to_netcdf(f'diagnostics/loss_data_{channel}_nonocean.nc', engine='netcdf4')\n",
    "\n",
    "    print(f'Finished training model for channel {channel}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
