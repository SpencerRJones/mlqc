{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ed9863-7a3a-49f2-b4eb-3f1cb466f908",
   "metadata": {},
   "source": [
    "# Channel Prediction Model for Quality Control\n",
    "\n",
    "This notebook contains the code and outlines the development of the\n",
    "ATMS Channel Prediction Neural Network for QC purposes of ATMS L1C\n",
    "data. The simple Feed-Forward Neural Networks (FFNNs) are trained on\n",
    "good quality ATMS brightness temperature observation vectors and are\n",
    "trained to predict one channel from all the others as predictors.\n",
    "\n",
    "The end product is a tree of FFNNs, discretized by surface (ocean or \n",
    "land), and by channel, since these are fully distinct problems."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b8054c4-bc31-4cbb-8e0d-76d0104d28d9",
   "metadata": {},
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import cartopy.crs as ccrs\n",
    "from util_funcs.L1C import scantime2datetime\n",
    "from util_funcs import data2xarray, array_funcs\n",
    "import geography\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset_class import dataset\n",
    "from model_class import channel_predictor\n",
    "import local_functions\n",
    "import sensor_info\n",
    "\n",
    "\n",
    "#################################################################\n",
    "scan_position = 0\n",
    "#################################################################\n",
    "\n",
    "\n",
    "#General parameters:\n",
    "\n",
    "satellite = sensor_info.satellite\n",
    "sensor = sensor_info.sensor\n",
    "\n",
    "nchans = sensor_info.nchannels\n",
    "batch_size = 1000\n",
    "input_size = nchans - 1\n",
    "hidden_size = 256\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "'''\n",
    "Function Definitions:\n",
    "'''\n",
    "\n",
    "chan_desc = sensor_info.channel_descriptions\n",
    "\n",
    "\n",
    "def extract_channel(Tb_array, chan):\n",
    "\n",
    "    '''\n",
    "    Use in preparing training data.\n",
    "\n",
    "    Assumes Tb array is [m x n] where m (rows) are samples\n",
    "    and n (columns) is the number of channels.\n",
    "\n",
    "    Passing in the channel description splits the data so\n",
    "    that the specified channel is its own vector y and the\n",
    "    rest are kept as predictors x.\n",
    "\n",
    "    Inputs:\n",
    "        Tb_array    |  ndarray of Tbs\n",
    "        chan        |  string of channel name\n",
    "    Outputs:\n",
    "        x           |  matrix of predictors (other channels)\n",
    "        y           |  vector of predictands (the missing channel)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    chan_desc = np.array(sensor_info.channel_descriptions)\n",
    "\n",
    "\n",
    "    chan_indx = np.where(chan == chan_desc)[0]\n",
    "\n",
    "    if np.size(chan_indx) == 0:\n",
    "        raise ValueError(f'Channel description must be in list {chan_desc}.')\n",
    "\n",
    "    y = Tb_array[:,chan_indx]\n",
    "    x = Tb_array[:,np.delete(np.arange(0,len(chan_desc)),chan_indx)]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train_model(model, nepochs, dataloader, learning_rate=0.001, quiet=False, stage=None, validation_dataloader=None):\n",
    "\n",
    "    nbatches = len(dataloader)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs)\n",
    "    device = None\n",
    "\n",
    "    if stage:\n",
    "        print(f'Training stage: {stage}')\n",
    "\n",
    "    loss_arr    = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    valloss_arr = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for i, (profs, obs) in enumerate(dataloader):\n",
    "            if device:\n",
    "                profs, obs = profs.to(device), obs.to(device)\n",
    "\n",
    "            if validation_dataloader and i%1000==0:\n",
    "                valprofs, valobs = next(enumerate(validation_dataloader))[1]\n",
    "                val_pred = model(valprofs)\n",
    "                valloss  = criterion(val_pred, valobs)\n",
    "                print(f'Validation Loss = {valloss.item():.3f}')\n",
    "\n",
    "            #Forward pass:\n",
    "            obs_pred = model(profs)\n",
    "            loss     = criterion(obs_pred, obs)\n",
    "\n",
    "\n",
    "            #Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #Update neurons:\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_arr[i,epoch] = loss.item()\n",
    "            valloss_arr[i,epoch] = valloss.item()\n",
    "            \n",
    "            if not quiet:\n",
    "                if i%1000 == 0:\n",
    "                    print(f'Epoch={epoch+1}, batch = {i} of {nbatches}, loss={loss.item():.3f}, LR={scheduler.get_last_lr()[0]}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    return loss_arr, valloss_arr\n",
    "\n",
    "'''\n",
    "\n",
    "SSMI CHANNEL PREDICTION MODEL:\n",
    "    1: Ocean\n",
    "\n",
    "'''\n",
    "\n",
    "sfc = [1]\n",
    "\n",
    "with xr.open_dataset(f'training_data/{satellite}_training_data.nc') as f:\n",
    "    \n",
    "    sfctype = f.sfctype.values\n",
    "\n",
    "    correct_sfc = np.isin(sfctype, sfc)\n",
    "    sfcindcs = np.where(correct_sfc)[0]\n",
    "\n",
    "    Tbs = f.Tbs.values[sfcindcs]\n",
    "    scanpos = f.scanpos.values[sfcindcs]\n",
    "    eia = f.eia.values[sfcindcs]\n",
    "\n",
    "correct_scanpos = scanpos == scan_position\n",
    "\n",
    "Tbs = Tbs[correct_scanpos]\n",
    "eia = eia[correct_scanpos]\n",
    "\n",
    "# if Tbs.shape[0] > 5.0e+06:\n",
    "#    Tbs = Tbs[:5_000_000,:]\n",
    "\n",
    "print(f'Training data shape: {Tbs.shape}')\n",
    "\n",
    "#---Split data into train/test/val:\n",
    "train_indcs, test_indcs, val_indcs = local_functions.split_data_indcs(Tbs)\n",
    "\n",
    "Tbs_train = Tbs[train_indcs]\n",
    "Tbs_test  = Tbs[test_indcs]\n",
    "Tbs_val   = Tbs[val_indcs]\n",
    "\n",
    "#---Shuffle before converting to tensors:\n",
    "np.random.seed(40)\n",
    "Tbs_train = array_funcs.shuffle_data(Tbs_train, axis=0)\n",
    "\n",
    "\n",
    "'''\n",
    "Predict channels: Train all models\n",
    "'''\n",
    "\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "for ichan, channel in enumerate(chan_desc):\n",
    "    \n",
    "    print(channel)\n",
    "\n",
    "    #---Extract channel, x = predictors, y = channel to predict\n",
    "    x_train, y_train = extract_channel(Tbs_train, channel)\n",
    "    x_test,  y_test  = extract_channel(Tbs_test, channel)\n",
    "    x_val,   y_val   = extract_channel(Tbs_val, channel)\n",
    "    \n",
    "    x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "    x_test,  y_test  = torch.tensor(x_test), torch.tensor(y_test)\n",
    "    x_val,   y_val   = torch.tensor(x_val), torch.tensor(y_val)\n",
    "\n",
    "\n",
    "    #---Set up dataloaders:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset(x_train,y_train), \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=dataset(x_test,y_test), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=dataset(x_val,y_val), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "\n",
    "    #---Create model:\n",
    "    model = channel_predictor(input_size, hidden_size, output_size)\n",
    "\n",
    "    #---Train_model:\n",
    "    nbatches = len(train_loader)\n",
    "    nepochs_stage1 = 10\n",
    "    nepochs_stage2 = 20\n",
    "    nepochs_stage3 = 40\n",
    "\n",
    "    loss_stage1, valloss_stage1 = train_model(model, nepochs=nepochs_stage1, dataloader=train_loader, \n",
    "                                          learning_rate=0.01, quiet=False, stage=1, validation_dataloader=val_loader)\n",
    "    loss_stage2, valloss_stage2 = train_model(model, nepochs=nepochs_stage2, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=2, validation_dataloader=val_loader)\n",
    "    loss_stage3, valloss_stage3 = train_model(model, nepochs=nepochs_stage3, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=3, validation_dataloader=val_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/{sensor}_{satellite}_channel_predictor_{channel}_scanpos{scan_position}_ocean.pt')\n",
    "\n",
    "    loss_data = data2xarray(data_vars = (loss_stage1, loss_stage2, loss_stage3, \n",
    "                                     valloss_stage1, valloss_stage2, valloss_stage3),\n",
    "                        var_names = ('LossStage1','LossStage2','LossStage3',\n",
    "                                     'ValidationLossStage1', 'ValidationLossStage2', 'ValidationLossStage3'),\n",
    "                        dims = (nbatches,nepochs_stage1,nepochs_stage2, nepochs_stage3),\n",
    "                        dim_names = ('training_batches', 'epochs_stage1', 'epochs_stage2', 'epochs_stage3'))\n",
    "\n",
    "    loss_data.to_netcdf(f'diagnostics/loss_data_{channel}_scanpos{scan_position}ocean.nc', engine='netcdf4')\n",
    "\n",
    "    print(f'Finished training model for channel {channel}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4384f412-5ad6-4c72-8376-a27a864600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import cartopy.crs as ccrs\n",
    "from util_funcs.L1C import scantime2datetime\n",
    "from util_funcs import data2xarray, array_funcs\n",
    "import geography\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset_class import dataset\n",
    "from model_class import channel_predictor\n",
    "import local_functions\n",
    "import sensor_info\n",
    "\n",
    "\n",
    "#General parameters:\n",
    "\n",
    "satellite = sensor_info.satellite\n",
    "sensor = sensor_info.sensor\n",
    "\n",
    "nchans = sensor_info.nchannels\n",
    "batch_size = 1000\n",
    "input_size = nchans - 1 #nchans - 1\n",
    "hidden_size = 256\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c8d88-3466-460c-b6eb-e222ce18441f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d6d779-b40f-41b0-a040-bfbcfdb0e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Definitions:\n",
    "'''\n",
    "\n",
    "chan_desc = sensor_info.channel_descriptions\n",
    "\n",
    "\n",
    "def extract_channel(Tb_array, chan):\n",
    "\n",
    "    '''\n",
    "    Use in preparing training data.\n",
    "\n",
    "    Assumes Tb array is [m x n] where m (rows) are samples\n",
    "    and n (columns) is the number of channels.\n",
    "\n",
    "    Passing in the channel description splits the data so\n",
    "    that the specified channel is its own vector y and the\n",
    "    rest are kept as predictors x.\n",
    "\n",
    "    Inputs:\n",
    "        Tb_array    |  ndarray of Tbs\n",
    "        chan        |  string of channel name\n",
    "    Outputs:\n",
    "        x           |  matrix of predictors (other channels)\n",
    "        y           |  vector of predictands (the missing channel)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    chan_desc = np.array(sensor_info.channel_descriptions)\n",
    "\n",
    "\n",
    "    chan_indx = np.where(chan == chan_desc)[0]\n",
    "\n",
    "    if np.size(chan_indx) == 0:\n",
    "        raise ValueError(f'Channel description must be in list {chan_desc}.')\n",
    "\n",
    "    y = Tb_array[:,chan_indx]\n",
    "    x = Tb_array[:,np.delete(np.arange(0,len(chan_desc)),chan_indx)]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train_model(model, nepochs, dataloader, learning_rate=0.001, quiet=False, stage=None, validation_dataloader=None):\n",
    "\n",
    "    nbatches = len(dataloader)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs)\n",
    "    device = None\n",
    "\n",
    "    if stage:\n",
    "        print(f'Training stage: {stage}')\n",
    "\n",
    "    loss_arr    = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    valloss_arr = np.zeros([nbatches,nepochs], dtype='f')\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for i, (profs, obs) in enumerate(dataloader):\n",
    "            if device:\n",
    "                profs, obs = profs.to(device), obs.to(device)\n",
    "\n",
    "            if validation_dataloader and i%1000==0:\n",
    "                valprofs, valobs = next(enumerate(validation_dataloader))[1]\n",
    "                val_pred = model(valprofs)\n",
    "                valloss  = criterion(val_pred, valobs)\n",
    "                print(f'Validation Loss = {valloss.item():.3f}')\n",
    "\n",
    "            #Forward pass:\n",
    "            obs_pred = model(profs)\n",
    "            loss     = criterion(obs_pred, obs)\n",
    "\n",
    "\n",
    "            #Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #Update neurons:\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_arr[i,epoch] = loss.item()\n",
    "            valloss_arr[i,epoch] = valloss.item()\n",
    "            \n",
    "            if not quiet:\n",
    "                if i%1000 == 0:\n",
    "                    print(f'Epoch={epoch+1}, batch = {i} of {nbatches}, loss={loss.item():.3f}, LR={scheduler.get_last_lr()[0]}')\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return loss_arr, valloss_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23443841-8148-44ed-85c9-212c5c930ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a6b8f7-502e-4436-82f6-c30c941a09ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2396650, 9)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "SSMI CHANNEL PREDICTION MODEL:\n",
    "    1: Ocean\n",
    "\n",
    "'''\n",
    "\n",
    "sfc = [1]\n",
    "\n",
    "with xr.open_dataset(f'training_data/{satellite}_training_data.nc') as f:\n",
    "    \n",
    "    sfctype = f.sfctype.values\n",
    "\n",
    "    correct_sfc = np.isin(sfctype, sfc)\n",
    "    sfcindcs = np.where(correct_sfc)[0]\n",
    "\n",
    "    Tbs = f.Tbs.values[sfcindcs]\n",
    "    scanpos = f.scanpos.values[sfcindcs]\n",
    "    eia = f.eia.values[sfcindcs]\n",
    "\n",
    "correct_scanpos = scanpos == 0\n",
    "#correct_eia = np.logical_and(eia > 45., eia < 55.)\n",
    "\n",
    "Tbs = Tbs[correct_scanpos]\n",
    "eia = eia[correct_scanpos]\n",
    "#Tbs = Tbs[correct_eia]\n",
    "#eia = eia[correct_eia]\n",
    "\n",
    "if Tbs.shape[0] > 5.0e+06:\n",
    "   Tbs = Tbs[:5_000_000,:]\n",
    "\n",
    "\n",
    "print(Tbs.shape)\n",
    "\n",
    "#---Split data into train/test/val:\n",
    "train_indcs, test_indcs, val_indcs = local_functions.split_data_indcs(Tbs)\n",
    "\n",
    "Tbs_train = Tbs[train_indcs]\n",
    "Tbs_test  = Tbs[test_indcs]\n",
    "Tbs_val   = Tbs[val_indcs]\n",
    "\n",
    "#scanpos_train = scanpos[train_indcs].astype(np.float32)\n",
    "#scanpos_test  = scanpos[test_indcs].astype(np.float32)\n",
    "#scanpos_val   = scanpos[val_indcs].astype(np.float32)\n",
    "\n",
    "eia_train = eia[train_indcs].astype(np.float32)\n",
    "\n",
    "#---Shuffle before converting to tensors:\n",
    "np.random.seed(40)\n",
    "Tbs_train = array_funcs.shuffle_data(Tbs_train, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e82dc-4ff0-4561-9536-9bed41a69932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11fad1e-8332-428e-a096-5023a880d7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predict channels: Train all models\n",
    "'''\n",
    "\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "for ichan, channel in enumerate(chan_desc):\n",
    "\n",
    "    if ichan < 2: continue\n",
    "    \n",
    "    print(channel)\n",
    "\n",
    "    #---Extract channel, x = predictors, y = channel to predict\n",
    "    x_train, y_train = extract_channel(Tbs_train, channel)\n",
    "    x_test,  y_test  = extract_channel(Tbs_test, channel)\n",
    "    x_val,   y_val   = extract_channel(Tbs_val, channel)\n",
    "    \n",
    "    x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "    x_test,  y_test  = torch.tensor(x_test), torch.tensor(y_test)\n",
    "    x_val,   y_val   = torch.tensor(x_val), torch.tensor(y_val)\n",
    "\n",
    "\n",
    "    #---Set up dataloaders:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset(x_train,y_train), \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=dataset(x_test,y_test), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=dataset(x_val,y_val), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "\n",
    "    #---Create model:\n",
    "    model = channel_predictor(input_size, hidden_size, output_size)\n",
    "\n",
    "    #---Train_model:\n",
    "    nbatches = len(train_loader)\n",
    "    nepochs_stage1 = 10\n",
    "    nepochs_stage2 = 20\n",
    "    nepochs_stage3 = 40\n",
    "\n",
    "    loss_stage1, valloss_stage1 = train_model(model, nepochs=nepochs_stage1, dataloader=train_loader, \n",
    "                                          learning_rate=0.01, quiet=False, stage=1, validation_dataloader=val_loader)\n",
    "    loss_stage2, valloss_stage2 = train_model(model, nepochs=nepochs_stage2, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=2, validation_dataloader=val_loader)\n",
    "    loss_stage3, valloss_stage3 = train_model(model, nepochs=nepochs_stage3, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=3, validation_dataloader=val_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/{sensor}_{satellite}_channel_predictor_{channel}_ocean.pt')\n",
    "\n",
    "    loss_data = data2xarray(data_vars = (loss_stage1, loss_stage2, loss_stage3, \n",
    "                                     valloss_stage1, valloss_stage2, valloss_stage3),\n",
    "                        var_names = ('LossStage1','LossStage2','LossStage3',\n",
    "                                     'ValidationLossStage1', 'ValidationLossStage2', 'ValidationLossStage3'),\n",
    "                        dims = (nbatches,nepochs_stage1,nepochs_stage2, nepochs_stage3),\n",
    "                        dim_names = ('training_batches', 'epochs_stage1', 'epochs_stage2', 'epochs_stage3'))\n",
    "\n",
    "    loss_data.to_netcdf(f'diagnostics/loss_data_{channel}_ocean.nc', engine='netcdf4')\n",
    "\n",
    "    print(f'Finished training model for channel {channel}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b23b0d-b0fb-44ab-ad7f-4e401aac2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "10, 40, 60: ~5.03, 4.795, 6.9, 1.073, 0.29, 0.177, 0.304, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90014cca-b376-44f2-928d-e66ea0e4ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "10, 20, 40: ~5.175, 3.248, 8.067, 1.077, 0.324, 0.174, 0.262, 0.292, 1.031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bbfc1f-8aa2-4772-a0de-2151e455d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General parameters:\n",
    "input_size = nchans - 1 + 1 #Remaining channels + surface type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286d562f-2684-4b6a-ac79-0aa7c9a343b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "SSMI CHANNEL PREDICTION MODEL:\n",
    "    2: Non-Ocean Surfaces\n",
    "\n",
    "'''\n",
    "\n",
    "with xr.open_dataset(f'training_data/{satellite}_training_data.nc') as f:\n",
    "    \n",
    "    sfctype = f.sfctype.values\n",
    "\n",
    "    correct_sfc = sfctype > 1\n",
    "\n",
    "    Tbs = f.Tbs.values[correct_sfc,:]\n",
    "    sfctype = sfctype[correct_sfc]\n",
    "\n",
    "#---Split data into train/test/val:\n",
    "train_indcs, test_indcs, val_indcs = local_functions.split_data_indcs(Tbs)\n",
    "\n",
    "Tbs_train = Tbs[train_indcs]\n",
    "Tbs_test  = Tbs[test_indcs]\n",
    "Tbs_val   = Tbs[val_indcs]\n",
    "\n",
    "sfctype_train = sfctype[train_indcs].astype(np.float32)\n",
    "sfctype_test  = sfctype[test_indcs].astype(np.float32)\n",
    "sfctype_val   = sfctype[val_indcs].astype(np.float32)\n",
    "\n",
    "#---Shuffle before converting to tensors:\n",
    "np.random.seed(40)\n",
    "Tbs_train, shuffled_indcs = array_funcs.shuffle_data(Tbs_train, axis=0, return_indcs=True)\n",
    "sfctype_train = sfctype_train[shuffled_indcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a135f0-542e-47c9-ace0-4d035ab8bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11., 11., 11., ...,  4.,  7., 17.],\n",
       "       shape=(78556753,), dtype=float32),\n",
       " array([2., 2., 2., ..., 2., 2., 2.], shape=(9819594,), dtype=float32),\n",
       " array([2., 2., 2., ..., 2., 2., 2.], shape=(9819595,), dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfctype_train, sfctype_test, sfctype_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da405ef-aa1e-4d67-8c1e-b5227b942921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predict channels: Train all models\n",
    "'''\n",
    "\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "for ichan, channel in enumerate(chan_desc):\n",
    "\n",
    "    print(channel)\n",
    "\n",
    "    #---Extract channel, x = predictors, y = channel to predict\n",
    "    x_train, y_train = extract_channel(Tbs_train, channel)\n",
    "    x_test,  y_test  = extract_channel(Tbs_test, channel)\n",
    "    x_val,   y_val   = extract_channel(Tbs_val, channel)\n",
    "\n",
    "    x_train = np.concatenate((x_train, sfctype_train[:,None]), axis=1)\n",
    "    x_test  = np.concatenate((x_test,  sfctype_test[:,None]), axis=1)\n",
    "    x_val   = np.concatenate((x_val,   sfctype_val[:,None]), axis=1)\n",
    "    \n",
    "    x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "    x_test,  y_test  = torch.tensor(x_test), torch.tensor(y_test)\n",
    "    x_val,   y_val   = torch.tensor(x_val), torch.tensor(y_val)\n",
    "\n",
    "\n",
    "    #---Set up dataloaders:\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset(x_train,y_train), \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=dataset(x_test,y_test), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=dataset(x_val,y_val), \n",
    "                                               batch_size=None, \n",
    "                                               shuffle=False, drop_last=False)\n",
    "\n",
    "    #---Create model:\n",
    "    model = channel_predictor(input_size, hidden_size, output_size)\n",
    "\n",
    "    #---Train_model:\n",
    "    nbatches = len(train_loader)\n",
    "    nepochs_stage1 = 5\n",
    "    nepochs_stage2 = 10\n",
    "    nepochs_stage3 = 20\n",
    "\n",
    "    loss_stage1, valloss_stage1 = train_model(model, nepochs=5, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=1, validation_dataloader=val_loader)\n",
    "    loss_stage2, valloss_stage2 = train_model(model, nepochs=10, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=2, validation_dataloader=val_loader)\n",
    "    loss_stage3, valloss_stage3 = train_model(model, nepochs=20, dataloader=train_loader, \n",
    "                                          learning_rate=0.001, quiet=False, stage=3, validation_dataloader=val_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'models/{sensor}_{satellite}_channel_predictor_{channel}_nonocean.pt')\n",
    "\n",
    "    loss_data = data2xarray(data_vars = (loss_stage1, loss_stage2, loss_stage3, \n",
    "                                     valloss_stage1, valloss_stage2, valloss_stage3),\n",
    "                        var_names = ('LossStage1','LossStage2','LossStage3',\n",
    "                                     'ValidationLossStage1', 'ValidationLossStage2', 'ValidationLossStage3'),\n",
    "                        dims = (nbatches,nepochs_stage1,nepochs_stage2, nepochs_stage3),\n",
    "                        dim_names = ('training_batches', 'epochs_stage1', 'epochs_stage2', 'epochs_stage3'))\n",
    "\n",
    "    loss_data.to_netcdf(f'diagnostics/loss_data_{channel}_nonocean.nc', engine='netcdf4')\n",
    "\n",
    "    print(f'Finished training model for channel {channel}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0a22e-5a4a-4571-8141-ec47273559de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
